# ML Notes for CV

This file will contain some Machine Learning(ML) knowledge for Computer Vision(CV).

***
# Table

+ [Decision Tree](#decision-tree)
+ [Support Vector Machine](#support-vector-machine)
+ [Feature Extraction and Sparse Learning](#feature-extraction-and-sparse-learning)
+ 


***
# Preface

在阅读CV的论文时，经常会看到一些机器学习的概念。这篇笔记的目的就是记录常见的机器学习概念，便于查阅。

***
# Main Body

## Decision Tree

![image](https://user-images.githubusercontent.com/36061421/125217876-4e63e800-e2f4-11eb-97f7-311a819566cf.png)

形如上面的样式，就是决策树的基本样式。

决策树学习的基本算法：

![image](https://user-images.githubusercontent.com/36061421/125217908-5cb20400-e2f4-11eb-9a8b-4ac81dd75e91.png)

对于决策树的生成，最重要的是第8行：选择最优的划分属性。

用下式定义`信息熵`，度量样本集合的纯度。

![image](https://user-images.githubusercontent.com/36061421/125218065-a995da80-e2f4-11eb-84bc-897864b01acf.png)

数值越小，纯度越高。

用下式表达信息增益：

![image](https://user-images.githubusercontent.com/36061421/125218076-b5819c80-e2f4-11eb-83ed-20947061e211.png)

一般而言，信息增益越大，意味着使用属性a来进行划分，获得的“纯度提升”越大。

除了这种度量方法，还有别的，比如：增益率和基尼指数。详见《机器学习》（周志华）P77。

从生成之后的决策树可以看出，这棵树很容易overfitting。这是因为这棵树学习到的东西包含了较多数据集独有的特征，这些特征并不具有代表性。为了降低overfitting的影响，可以进行剪枝（pruning）操作。

剪枝分为两种：1.预剪枝；2.后剪枝。

**预剪枝**就是在决策树一步步成型的过程中，进行划分前后的精度比较。如果划分后精度可以提升，那么可以继续生成，否则令当下结点为叶子结点，结束生成。

![image](https://user-images.githubusercontent.com/36061421/125218459-a18a6a80-e2f5-11eb-84bf-2ae2214bd3aa.png)

精度的判别就是让测试集在训练集的基础上进行测试。基于训练集，有了整棵树的下一步规划，这时候拿测试集数据来比较分支前后的精度变化。

可以看出，这种做法在一定程度上可以降低过拟合的风险，此外还可以降低运算复杂度。但由于这种方法是基于贪心的本质，所以最终的结果未必是最佳的，可能有欠拟合的风险，同样导致泛化能力不佳。

**后剪枝**就是在决策树生成之后，进行剪枝。剪枝与否的标准也是在于验证集精度的变化。如果精度提升，那么可以剪枝，反之不剪。

![image](https://user-images.githubusercontent.com/36061421/125219089-cc28f300-e2f6-11eb-82a9-b58e9cab1e40.png)

后剪枝可以保留更多分支，欠拟合的风险减小。与此同时，泛化能力比较好。但最大的问题是计算开销大。

## Support Vector Machine
Pending...

## Feature Extraction and Sparse Learning

由于数据量的庞大，我们可能遇到“维数爆炸”的问题。在机器学习的过程中，并不是所有数据都在起巨大作用。在海量的数据中，我们应该挑选“有意义”的数据。因此，数据降维很重要。

数据降维一般可以采用两种手段：1.PCA等降维手段；2.特征选择。这里介绍特征选择，PCA以后介绍。

特征选择一般分为三种：
+ 1.过滤式选择：过滤式方法先对数据集进行特征选择，然后再训练学习器，特征选择过程与后续学习器无关。这相当于先用特征选择过程对初始特征进行"过滤"，再用过滤后的特征来训练模型。知名方法有Relief算法，这是针对二分类问题的；
+ 2.包裹式选择：与过滤式特征选择不考虑后续学习器不间，包裹式特征选择直接把最终将要使用的学习器的性能作为特征于集的评价准则。换言之，包裹式特征选择的目的就是为给定学习器选择最有利于其性能、"量身定做"的特征子集。LVW (Las Vegas Wrapper)是一个典型的包裹式特征选择方法。它在拉斯维加斯方法(Las Vegas method)框架下使用随机策略来进行子集搜索，并以最终分类器的误差为特征子集评价准则；
+ 3.嵌入式选择：在过滤式和包裹式特征选择方法中，特征选择过程与学习器训练过程有明显的分别;与此不同，嵌入式特征选择是将特征选择过程与学习器训练过程融为一体，两者在同一个优化过程中完成，即在学习器训练过程中自动地进行了特征选择。（在嵌入式选择的章节中，还有涉及`岭回归`等内容。）

详细内容参见《机器学习》（周志华）P249。

**稀疏表示和字典学习**

前面的“稀疏”，指的是和学习任务无关，在特征选择过程中应该被剔除掉的内容。在这里，“稀疏”的含义有些不同。指的是在矩阵中，很多元素为0。这些0元素并不是规律地成行或者成列出现。

那么，研究这种“稀疏”有什么意义？

当样本具有这样的稀疏表达形式时，对学习任务来说会有不少好处，例如：线性支持向量机之所以能在文本数据上有很好的性能，恰是由于文本数据在使用上述的字频表示后具有高度的稀疏性，使大多数问题变得线性可分。同时，稀疏样本并不会造成存储上的巨大负担，因为稀疏矩阵己有很多高效的存储方法。

值得注意的是：我们要的“稀疏”应该是适当的。举例来说，如果在从事文档分类的任务，那么《康熙字典》就是“过度稀疏”，《现代汉语常用字表》就是“恰当稀疏”。

显然，在一般的学习任务中(例如图像分类)并没有《现代汉语常用字表》可用，我们需学习出这样一个"字典"为普通稠密表达的样本找到合适的字典，将样本转化为合适的稀疏表示形式，从而使学习任务得以简化，模型复杂度得以降低，通常称为"字典学习" (dictionary learning) ，亦称"稀疏编码" (sparse coding)。

这两个称谓稍有差别，"字典学习"更侧重于学得字典的过程，而"稀疏编码"则更侧重于对样本进行稀疏表达的过程。由于两者通常是在同一个优化求解过程中完成的，因此下面我们不做进一步区分，笼统地称为字典学习。

给定数据集{Xl，X2，... ，Xm}，字典学习最简单的形式为：

![image](https://user-images.githubusercontent.com/36061421/125221392-c0d7c680-e2fa-11eb-8712-b4969e8177bf.png)

其中B∈R 为字典矩阵，k称为字典的词汇量，通常由用户指定， αi∈Rk 则是样本x∈Rd的稀疏表示。显然，上式的第一项是希望由α能很好地重构Xi，第二项则是希望αi尽量稀疏.

求解过程见P256。

**压缩感知**

现实生活中，希望通过少量信息重构全部信息。对于通信系统，如果想要恢复原始信号，需要满足奈奎斯特采样定理。那可否用一些方式，突破奈奎斯特采样定理，使用更少的内容恢复原始信号？这就是压缩感知要做的事情。

事实上，在很多应用中均可获得具有稀疏性的信号，例如图像或声音的数字信号通常在时域上不具有稀疏性，但经过傅里叶变换、余弦变换、小波变换等处理后却会转化为频域上的稀疏信号。









